import cv2 as cv
import numpy as np
import time
import math
import serial

arduino = serial.Serial('COM7', 115200)
time.sleep(2)

CODEWORD = [0,0,0,0]

scale_down = 0.65


mid_length = int(110 * scale_down)
circle_radius = 25

# Import Pre-trained architecture and weights
PROTOTXT = "MobileNetSSD_deploy (2).prototxt"
MODEL = "MobileNetSSD_deploy (2).caffemodel"

# Class list
CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable",
           "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
# Creates a 2D numpy array that allocates each class with a corresponding RGB value
COLOURS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

# Loads a pre-trained neural network model using Caffe framework, returns the loaded neural network
net = cv.dnn.readNetFromCaffe(PROTOTXT, MODEL)

# Initialize webcam (set to 0 for default webcam)
cap = cv.VideoCapture(0)


fps = 0
frame_count = 0

# Records the start time of the processing loop
start_time = time.time()


circle_center_x = int(352 * scale_down)
circle_center_y = int(336 * scale_down)

circle_center = (circle_center_x, circle_center_y)

tracking = False

while True:

    ret, frame_big = cap.read()


    frame = cv.resize(frame_big, None, fx= scale_down, fy= scale_down, interpolation= cv.INTER_AREA)

    if (tracking == False):
            
        CODEWORD = [0, 0, 0, 0]

        cv.putText(frame, "detecting", (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

        (h, w) = frame.shape[:2]

       

        # Preprocess the frame for object detection
        blob = cv.dnn.blobFromImage(cv.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)
        net.setInput(blob)

        # Perform forward pass to get detections
        detections = net.forward()

        # Process the first detection
        confidence = detections[0, 0, 0, 2]

        # Check if the first detection has high confidence
        if confidence > 0.9:
            idx = int(detections[0, 0, 0, 1])

            # Check if the detected object is a person
            if CLASSES[idx] == "person":

                # Extract the bounding box for the first detection
                box = detections[0, 0, 0, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")
                width = endX - startX
                height = endY - startY
                bbox = (startX, startY, width, height)

                # Initialize tracker on the detected person
                tracker = cv.legacy.TrackerKCF_create()
                tracker.init(frame, bbox)
                tracking = True

        

    elif (tracking == True):
        
        success, bbox = tracker.update(frame)

        if success:
            
            real_x, real_y, real_w, real_h = [int(v) for v in bbox]
    

            MiddleX = real_x + (real_w // 2)
            MiddleY = real_y + (real_h // 2)

            startX = MiddleX - mid_length
            endX = MiddleX + mid_length

            startY = MiddleY - mid_length
            endY = MiddleY + mid_length

            cv.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)

            cv.circle(frame, (MiddleX, MiddleY), circle_radius, (255, 0, 0), thickness = 2)

            cv.putText(frame, "tracking", (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            CODEWORD[0] = 1  # Set first codeword digit to indicate detection

            # Determine if the person is aligned horizontally with the center of the screen
            if MiddleX - circle_radius <= circle_center_x <= MiddleX + circle_radius:
                CODEWORD[1] = 1  # Person is aligned horizontally
            elif circle_center_x > MiddleX + circle_radius:  # Move right
                CODEWORD[1] = 2
            else:  # Move left
                CODEWORD[1] = 3
        
            # Determine if the person is aligned vertically with the center of the screen
            if MiddleY - circle_radius <= circle_center_y <= MiddleY + circle_radius:
                CODEWORD[2] = 1  # Person is aligned vertically
            elif circle_center_y > MiddleY + circle_radius:  # Move down
                CODEWORD[2] = 2
            else:  # Move up
                CODEWORD[2] = 3

            # Check if the person is fully inside the firing zone
            if startX <= circle_center_x <= endX and startY <= circle_center_y <= endY:
                CODEWORD[3] = 1  # Ready to fire
            else:
                CODEWORD[3] = 0  # Not ready to fire


        else:
            tracking = False


    string_CODEWORD = ''.join(map(str, CODEWORD))

    print(string_CODEWORD)
    arduino.write((string_CODEWORD + '\n').encode())


    frame_count += 1
    elapsed_time = time.time() - start_time
    if elapsed_time > 0:
        fps = frame_count / elapsed_time

    fps_text = f"FPS: {fps:.2f}"
    cv.putText(frame, fps_text, (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv.circle(frame, circle_center, 15, (0, 255, 0), thickness=2)
    cv.imshow("tracking", frame)

    if cv.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv.destroyAllWindows()
arduino.close()





Key Components:
Libraries and Initialization:

cv2, numpy, serial, and time are imported for computer vision, numerical operations, Arduino communication, and time delays, respectively.
An Arduino board is initialized via a serial connection (COM7 at 115200 baud rate).
Some global variables are defined, such as the CODEWORD array, which is used to communicate with the Arduino, and constants like circle_radius, mid_length, and center positions for tracking.
Loading the Pre-trained Model:

The code loads the pre-trained MobileNetSSD model using Caffe framework (MobileNetSSD_deploy (2).prototxt and MobileNetSSD_deploy (2).caffemodel). This neural network can detect various object classes, including "person", "car", "dog", etc.
CLASSES stores the list of object categories the model can detect.
Webcam Capture:

cap = cv.VideoCapture(0) initializes the default webcam for capturing video frames.
Main Loop:

The code runs an infinite loop to capture frames from the webcam.
It starts by attempting to detect objects (specifically a person) when tracking is set to False. Once a person is detected, the system starts tracking them using OpenCVâ€™s KCF tracker (Kernelized Correlation Filters).
Object Detection:

Each frame is converted into a "blob" using cv.dnn.blobFromImage(), which preprocesses the image (resizing, scaling, and mean subtraction).
The blob is passed through the neural network (net.setInput() and net.forward()) to detect objects in the frame. If the confidence score for any detection is higher than 0.75, and the detected object is a "person", the system proceeds with tracking.
Tracking:

Once a person is detected, the code initializes the KCF tracker on the bounding box of the detected person.
The system tracks the person's movement across frames using tracker.update(). If tracking is successful, it draws a rectangle around the detected person and computes the middle coordinates of the bounding box.
Control Logic:

Horizontal and Vertical Alignment: The system compares the detected person's position to the centre of the screen (circle at (352, 336)). It determines if the person is aligned horizontally and vertically within a certain radius (circle_radius). Based on the alignment, it updates the CODEWORD array:
CODEWORD[0]: Indicates if a person is detected (1 if detected).
CODEWORD[1]: Direction control for horizontal alignment (1 if aligned, 2 to move right, 3 to move left).
CODEWORD[2]: Direction control for vertical alignment (1 if aligned, 2 to move down, 3 to move up).
CODEWORD[3]: Indicates if the person is fully within the firing zone (1 if yes, 0 if no).
This CODEWORD is converted to a string and sent to the Arduino over the serial connection for further action (e.g., controlling a motor or firing mechanism).
Frames Per Second (FPS) Calculation:

The code calculates and displays the current FPS of the video processing loop to give feedback on system performance.
Exit Condition:

If the user presses the 'q' key, the program will stop the loop, release the webcam, close the Arduino connection, and destroy all OpenCV windows.
cv.destroyAllWindows()


